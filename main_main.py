import os 
import json
from uuid import uuid4


import requests
import numpy as np

from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv


load_dotenv()
HF_API_KEY=os.getenv("HF_API_KEY")
API_KEY=os.getenv("API_KEY")
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")
BASE_URL = 'https://openrouter.ai/api/v1'
MODEL = 'mistralai/mistral-small-3.1-24b-instruct'


embedding_model = SentenceTransformer("ai-forever/ru-en-RoSBERTa")


def simple_ask():
    client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

    user_prompt = "–ö—Ç–æ —Å–µ–π—á–∞—Å –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê?"
    # user_prompt = "–†–∞—Å—Å–∫–∞–∂–∏ –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã OOO –†–æ–º–∞—à–∫–∞ –≤ –≥–æ—Ä–æ–¥–µ –ö–∏—Ä–æ–≤"


    messages = [{"role": "user", "content": user_prompt}]

    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        max_tokens=8096,
        temperature=0.1
    )

    print(response.choices[0].message.content)


def context_ask():
    client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

    user_prompt = "–†–∞—Å—Å–∫–∞–∂–∏ –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã OOO –†–æ–º–∞—à–∫–∞ –≤ –≥–æ—Ä–æ–¥–µ –ö–∏—Ä–æ–≤"

    context = """
–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å 8 –¥–æ 20, —à–æ—É—Ä—É–º –Ω–∞ —É–ª.–õ–µ–Ω–∏–Ω–∞ 25, –ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏ —É–ª.–ü—É—à–∫–∏–Ω–∞ 17.
"""
    system_prompt = f"""
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–¥–∞–≤–µ—Ü-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. 
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äì –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å –æ—Ç–≤–µ—á–∞—Ç—å. –°–æ—Ö—Ä–∞–Ω—è–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ —É–≤–µ—Ä–µ–Ω–Ω—ã–π —Ç–æ–Ω.

**–†–æ–ª—å:**
- –≠–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º/—É—Å–ª—É–≥–∞–º –∫–æ–º–ø–∞–Ω–∏–∏
- –ú–∞—Å—Ç–µ—Ä –≤–µ–∂–ª–∏–≤–æ–≥–æ –æ–±—â–µ–Ω–∏—è
- –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: <CONTEXT_START>{context}<CONTEXT_END>
2. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å: {user_prompt}
3. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞: "–ò–∑–≤–∏–Ω–∏—Ç–µ, —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£—Ç–æ—á–Ω–∏—Ç–µ –¥–µ—Ç–∞–ª–∏ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞"
4. –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Ä–∞–∑–±–∏–≤–∞–π –æ—Ç–≤–µ—Ç –Ω–∞ –ø—É–Ω–∫—Ç—ã
5. –ò–∑–±–µ–≥–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∂–∞—Ä–≥–æ–Ω–∞

**–°—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–∞:**
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∂–∏ –¥–ª—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ (1-2 –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏)
- –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–π –≤—ã–≥–æ–¥—ã –∫–ª–∏–µ–Ω—Ç–∞
- –ü—Ä–µ–¥–ª–∞–≥–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: "–í–æ–∑–º–æ–∂–Ω–æ –≤–∞—Å —Ç–∞–∫–∂–µ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç..."
- –ó–∞–≤–µ—Ä—à–∞–π –≤–æ–ø—Ä–æ—Å–æ–º: "–•–æ—Ç–∏—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –¥–µ—Ç–∞–ª–∏?"

**–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:**
"–î–æ–±—Ä—ã–π –¥–µ–Ω—å! <–æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞>. 
–ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏? üòä"

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è:**
- –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤
- –ò–∑–±–µ–≥–∞–π markdown-—Ä–∞–∑–º–µ—Ç–∫–∏
- –û—Ç–≤–µ—Ç –¥–µ—Ä–∂–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
"""


    messages = [{"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}]

    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        max_tokens=8096,
        temperature=0.1
    )

    print(response.choices[0].message.content)

####################### work_with_embeddings ##############################

def get_embeddings_local(text, task = "search_document"):
    prefixed_text = f"{task}: {text}"
    # prefixed_text = text

    embedding = embedding_model.encode(
        prefixed_text,
        normalize_embeddings=True,
        convert_to_numpy=False,
        show_progress_bar=False
    ).tolist()
    return embedding


def get_embeddings_api(text, task = "search_query"):
    API_URL = "https://router.huggingface.co/hf-inference/models/ai-forever/ru-en-RoSBERTa/pipeline/feature-extraction"
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
    }
    prefixed_text = f"{task}: {text}"
    # prefixed_text = text
    payload = {
        "inputs": prefixed_text,
        "parameters": {
            "pooling_method": "cls",  # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É–ª–∏–Ω–≥–∞
            "normalize_embeddings": True
        }
    }
    
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

def embedd_example(text):
    print(get_embeddings_api(text))

def create_collection(client, collection_name, size):
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=size, distance=Distance.COSINE)
    )

def add_point(client, collection_name, text, payload):
    client.upsert(
    collection_name=collection_name,
    points=[
        PointStruct(
            id=str(uuid4().hex),
            vector=get_embeddings_local(text),
            payload=payload
        )
    ])

def make_chanks(long_text):
    splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\n\n","\n", ". ", "! ", "? ", ]
    )
    chunks = splitter.split_text(long_text)
    return chunks

def clear_text_to_embedding(text):
    client = OpenAI(
        base_url=BASE_URL,
        api_key=API_KEY
    )

    system_prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. 
–ü—Ä–æ–≤–µ–¥–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º:

1. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã**:
- –£–¥–∞–ª–∏ HTML/CSS/JS –∫–æ–¥, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ !?.,), –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
- –ó–∞–º–µ–Ω–∏ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ "
- –ü—Ä–∏–≤–µ–¥–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫ –ø–æ–ª–Ω–æ–π —Ñ–æ—Ä–º–µ: "–Ω-—Ä" ‚Üí "–Ω–∞–ø—Ä–∏–º–µ—Ä"

2. **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏–µ**:
- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ä–º–∏–Ω—ã, —Ü–∏—Ñ—Ä—ã, –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
- –£–¥–∞–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ ("–æ—á–µ–Ω—å", "–ø—Ä–æ—Å—Ç–æ") –∏ –≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã 
- –û–±—ä–µ–¥–∏–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ç–µ–º–æ–π
- –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏–µ: "Python: —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"

3. **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏—è**:
- –î–æ–±–∞–≤—å —è–≤–Ω—ã–µ —Å–≤—è–∑–∏: –≤–º–µ—Å—Ç–æ "—Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ" ‚Üí "—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: 500 –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫"
- –ó–∞–º–µ–Ω–∏ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã: "–æ–Ω" ‚Üí "–∞–ª–≥–æ—Ä–∏—Ç–º"
- –≠–∫—Å–ø–ª–∏—Ü–∏—Ä—É–π –∏–º–ø–ª–∏—Ü–∏—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ [–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö]

4. **–ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ** (–¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤):
- –†–∞–∑–±–µ–π –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ –ø–æ 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –î–æ–±–∞–≤—å —Ü–µ–ø–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã: "#1", "#2"
- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —á–∞–Ω–∫–∞

**–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:**

–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:
"–ù–∞—à —Å–µ—Ä–≤–∏—Å, –∫—Å—Ç–∞—Ç–∏, –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ 500 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å."

–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π:
"–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: 500 –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫. [–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞] #1"

**–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:**
- –¢–æ–ª—å–∫–æ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–º—ã—Å–ª–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
- –ß–∞–Ω–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π

–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:
"""

    messages = [{"role": "system", "content": system_prompt},
            {"role": "user", "content": text}]

    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        max_tokens=8096,
        temperature=0.1
    )
    result = response.choices[0].message.content
    print("-----clear_text_to_embedding-----")
    print('original text', text)
    print('---------------------------------')
    print('clear text', result)
    return result

############################ read_json ###############################

def read_json_and_add_point(client, collection_name):
    with open('./result.json', 'r') as file:
        data = json.loads(file.read())
        print('–≤—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π:', len(data.get('messages')))
        count = len(data.get('messages'))
        for mess in data.get('messages')[:10]:
            print('id messages:', mess.get('id'))
            count -=1
            print('–û—Å—Ç–∞–ª–æ—Å—å:', count)
            if mess.get('text_entities'):
                full_text = []
                for tex_en in mess.get('text_entities'):
                    if tex_en.get('type') == 'plain':
                        full_text.append(tex_en.get('text'))
                text_to_embedd = ' '.join(full_text)

                payload = {
                        "content": text_to_embedd,
                        "metadata": { 
                            "message_id": mess.get('id')
                        }}
                add_point(client, collection_name, text_to_embedd, payload)


def read_json_clear_and_add_point_v1(client, collection_name):
    with open('./result.json', 'r') as file:
        data = json.loads(file.read())
        print('–≤—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π:', len(data.get('messages')))
        count = len(data.get('messages'))
        for mess in data.get('messages'):
            print('id messages:', mess.get('id'))
            count -=1
            print('–û—Å—Ç–∞–ª–æ—Å—å:', count)
            if mess.get('text_entities'):
                full_text = []
                for tex_en in mess.get('text_entities'):
                    if tex_en.get('type') == 'plain':
                        # 1 way
                        clear_text = tex_en.get('text').replace('\n', ' ').strip().lower()
                        full_text.append(clear_text)

                text_to_embedd = ' '.join(full_text)

                # 2 way
                text_to_embedd = clear_text_to_embedding(text_to_embedd)

                payload = {
                        "content": text_to_embedd,
                        "metadata": { 
                            "message_id": mess.get('id')
                        }}
                add_point(client, collection_name, text_to_embedd, payload)


def read_json_clear_and_add_point_v2(client, collection_name):
    with open('./result.json', 'r') as file:
        data = json.loads(file.read())
        print('–≤—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π:', len(data.get('messages')))
        count = len(data.get('messages'))
        for mess in data.get('messages'):
            print('id messages:', mess.get('id'))
            count -=1
            print('–û—Å—Ç–∞–ª–æ—Å—å:', count)
            if mess.get('text_entities'):
                full_text = []
                for tex_en in mess.get('text_entities'):
                    if tex_en.get('type') == 'plain':
                        clear_text = tex_en.get('text').replace('\n', ' ').strip().lower()
                        full_text.append(clear_text)

                text_to_embedd = ' '.join(full_text)

                # 2 way
                text_to_embedd = clear_text_to_embedding(text_to_embedd)

                text_chanks = make_chanks(text_to_embedd)
        
                for chank in text_chanks:
                    payload = {
                            "content": text_to_embedd,
                            "metadata": { 
                                "message_id": mess.get('id')
                            }}
                    add_point(client, collection_name, chank, payload)


def find_text(client, collection_name, text):
    results = client.query_points(
        collection_name=collection_name,
        query=get_embeddings_api(text.lower()),
        limit=5,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with_payload=True  # –í–∫–ª—é—á–∏—Ç—å payload –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    )
    for _, scope in results:
        for point in scope:
            print(f"score: {point.score}")
            print(f"text: {point.payload.get('text')}")
            print("-----------")

if __name__ == "__main__":
    # simple_ask()
    # context_ask()
    # embedd_example('–ü—Ä–∏–≤–µ—Ç ')  


    client = QdrantClient(host="localhost", port=6333)
    collection_name = "rag_embeddings_with_prefix_clear_chanks_300_50"
    # text = '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞–≥–∞–∑–∏–Ω–∞ –º–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –±—ã—Å—Ç—Ä–æ'

    # text = '—Ç–æ—á–∫–∏ —Ä–æ—Å—Ç–∞ –±–∏–∑–Ω–µ—Å–∞'
    # text = '–†–∞–∑–≤–∏—Ç–∏–µ –ª–∏—á–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞'
    text = '–ó–∞—Ä–∞–±–æ—Ç–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω'
    # collection_name = 'no_clear_embeddings'
    find_text(client,collection_name, text)
    
    
    client.close()